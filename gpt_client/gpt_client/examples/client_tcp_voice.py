import os
import sys
import logging
import socket

from utils_asr import record, speech_recognition  # å¯¼å…¥è¯­éŸ³è¯†åˆ«æ¨¡å—
from utils_tts import tts, play_wav               # å¯¼å…¥è¯­éŸ³åˆæˆæ¨¡å—
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI  # ä¿®æ”¹è¿™é‡Œä»¥å¯¼å…¥è‡ªå®šä¹‰çš„ OpenAI å®¢æˆ·ç«¯

sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))
from gpt_client.gpt_client.prompts.prompt_template import QA_TEMPLATE_BAICHUAN
import gpt_client.gpt_client.commons.embedding_utils as eu
from gpt_client.gpt_client.commons.utils import *

logging.basicConfig(level=logging.INFO)


class GPTAssistant:
    """ Load ChatGPT config and your custom pre-prompts. """

    def __init__(self, verbose=False) -> None:
        
        logging.info("Loading keys...")
        cfg_file = os.path.join(os.path.dirname(__file__), '../commons/config_local.json')
        set_global_configs(cfg_file)
        logging.info(f"Done.")
        API_BASE = "http://ali.styin8.cn:7699/v1"

        logging.info("Initialize LLM...")
        # ä¿®æ”¹è¿™é‡Œåˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        llm = ChatOpenAI(
            model="medical_robot",
            base_url=API_BASE,
            temperature=0.1,
            max_tokens=2048,
        )
        logging.info(f"Done.")

        logging.info("Initialize tools...")
        embedding_model = eu.init_embedding_model()
        vector_store = eu.init_vector_store(embedding_model)
        logging.info(f"Done.")

        logging.info("Initialize chain...")
        chain_type_kwargs = {"prompt": QA_TEMPLATE_BAICHUAN, "verbose": verbose}
        self.conversation = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type='stuff',
            retriever=vector_store.as_retriever(search_kwargs={'k': 3}),
            chain_type_kwargs=chain_type_kwargs,
            return_source_documents=True
        )
        logging.info(f"Done.")

        os.system("clear")
        streaming_print_banner()

    def ask(self, question):
        result_dict = self.conversation(question)
        result = result_dict['result']
        return result


def main(args=None):
    IS_DUBUG = True
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    gpt = GPTAssistant(
        verbose=True,
    )
    play_wav('/home/gcj/robochain-main/gpt_client/gpt_client/examples/asset/welcome.wav')
    if not IS_DUBUG:
        HOST = '192.168.3.26'
        PORT = 5001
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((HOST, PORT))
        print("Connected to server.")

    while True:
        start_record_ok = input('æ˜¯å¦å¼€å¯å½•éŸ³, è¾“å…¥æ•°å­—å½•éŸ³æŒ‡å®šæ—¶é•¿, æŒ‰kæ‰“å­—è¾“å…¥\n')
        if start_record_ok.isnumeric():
            DURATION = int(start_record_ok)
            record(DURATION=DURATION)   # å½•éŸ³
            question = speech_recognition('/home/gcj/robochain-main/gpt_client/gpt_client/examples/temp/speech_record.wav') # è¯­éŸ³è¯†åˆ«
            print(question)
        elif start_record_ok == 'k':
            question = input(colors.YELLOW + "ç”¨æˆ·ðŸ’¬> " + colors.ENDC)
            if question == "!quit" or question == "!exit":
                break
            if question == "!clear":
                os.system("clear")
                continue
        else:
            print('æ— æŒ‡ä»¤ï¼Œé€€å‡º')
            raise NameError('æ— æŒ‡ä»¤ï¼Œé€€å‡º')

        result = gpt.ask(question)  # Ask a question
        print(colors.GREEN + "æœºå™¨äººðŸ¤–> " + colors.ENDC + f"{result}")
        if not IS_DUBUG:
            s.sendall(result.encode())


if __name__ == '__main__':
    main()
